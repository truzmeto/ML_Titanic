---
title: "Hit Top 9 Percentile"
author: "tar159"
date: "December 15, 2017"
output:
    html_document:
        code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
options(warn = -1)
```

## Introduction

We must regularly go to gym in order to stay fit. Same concept applies to brain muscles towards 
some specific knowledge. One must be active in kaggle to train his/her machine learning muscles.
Anyways, I've decided to do some deeper machine learning after taking some advanced ML courses. Looks
like titanic data set is the best and most popular data to start with. Here, I will exploit some machine
learning methods such as "knn", "Dtree", "prunnedTree", "gboosting", "nnet", "SVM", "rf" and "xgBoost" with
repeated cross validation that helps to choose best hyper parameter for an optimum prediction. Then, I combine
all predictions of test set to do majority vote. My finale prediction put me at top 9%, which is not bad for
starters, but it is possible to push it even more up by combining more models. 


```{r lib&data loading, echo=TRUE, message=FALSE, warning=FALSE}
## importing libraries
library("knitr")
library("ggplot2")
library("lattice") 
library("caret")
library("rpart")
library("rpart.plot")
library("RColorBrewer")
library("rattle")
library("randomForest")
library("lattice")
library("plyr")
library("Rmisc")
library("party")
library("doMC")
library("corrplot")
library("dplyr")
library("tidyr")
library("rpart.plot")

## loading locally stored data
test_set <- read.csv("../data/test.csv", na.strings = c("NA",""))
train_set <- read.csv("../data/train.csv", na.strings = c("NA",""))

test_set$Survived <- 2
## combining train and test sets
data <- rbind(train_set, test_set)
```


## Preprocessing, Data Cleaning & Feature Extraction

My preprocessing of unclean data is very similar to what most people have done. I need to give a credit
to Megan Risdal since I learned some tricks from her kernel. 

```{r preprocessing, message=FALSE, warning=FALSE}
#counting percentage of NA's for each feature
round(colSums(is.na(data))/nrow(data) *100,2)

#Cabin feature have 77.1% missing values, so let's ignore it.
# removing Cabin features from both train and test sets
data$Cabin <- NULL

## creating a new feature by extracting title from names, where we replace
## everything before ', ' and after '.' with '' (empty char.)
data$Titles <- gsub('(.*[,] )|([.].*)', '', data$Name)

## tabulating titles grouped by sex
#table(data$Sex, data$Titles)

## taking care of rare female titles
data$Titles[data$Titles == 'Ms'] <- 'Miss' 
data$Titles[data$Titles == 'Mme'] <- 'Mrs' 
data$Titles[data$Titles == 'Mlle'] <- 'Miss' 
data$Titles[data$Titles == 'Dona'] <- 'Miss' 
data[data$Titles == 'Lady',]$Titles <- 'Mrs'
data[data$Titles == 'the Countess',]$Titles <- 'Mrs'
data[data$Sex == 'female' & data$Titles == 'Dr',]$Titles <- 'Miss'

## renaming all rare male titles with most common male title
minor_title_male <- c('Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
data$Titles[data$Titles %in% minor_title_male]  <- 'Mr'

## here we construct values for missing ages by taking
## a mean for each name title. May not be the best way!
mean_age_by_title <- aggregate(Age ~ Titles, data = data, mean)
data[is.na(data$Age) & data$Titles == 'Master' ,]$Age <- mean_age_by_title$Age[1]
data[is.na(data$Age) & data$Titles == 'Miss' ,]$Age <- mean_age_by_title$Age[2]
data[is.na(data$Age) & data$Titles == 'Mr' ,]$Age <- mean_age_by_title$Age[3]
data[is.na(data$Age) & data$Titles == 'Mrs' ,]$Age <- mean_age_by_title$Age[4]

## filling missing fare by mean of passenger class
mean_fare_by_pclass <- aggregate(Fare ~ Pclass, data = data, mean)
data[is.na(data$Fare),]$Fare <- mean_fare_by_pclass$Fare[3]

## imputing missing embarkation type with highest embarkation class
data[is.na(data$Embarked),]$Embarked <- "S"

#kable(table(data$Sex, data$Titles), align = "l")

# age based feature ectraction
data$AdultTeenChi[data$Age <= 12.0] <- "Child"
data$AdultTeenChi[data$Age > 12.0 & data$Age <= 20.0] <- "Teen"
data$AdultTeenChi[data$Age > 20.0 & data$Age <= 35.0] <- "YoungAdult"
data$AdultTeenChi[data$Age > 35.0 & data$Age <= 50.0] <- "MiddleAged"
data$AdultTeenChi[data$Age > 50.0] <- "Siniors"
data$AdultTeenChi <- as.factor(data$AdultTeenChi)
#kable(table(data$AdultTeenChi, data$Titles), align = "l")

## recollecting the data back to train and test sets after
## cleaning and imputations are done
data$Titles <- as.factor(data$Titles)
data$Name <- NULL
data$Ticket <- NULL
test_set <- data[data$PassengerId >= 892,]
train_set <- data[data$PassengerId < 892,]
test_set$PassengerId <- NULL
train_set$PassengerId <- NULL
```

## Some statistics


```{r, eval=T, echo=T, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}

p1 <- ggplot(train_set, aes(x= Pclass, fill= factor(Survived))) +
            geom_histogram(binwidth=0.4, position="dodge")+
            theme_bw() +
            labs(title = "Survival by passenger class" ,
                    x="Passenger class", color="") +
            theme(axis.title = element_text(size = 16.0),
                    legend.position = c(0.3,0.8),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) +
            scale_fill_brewer(palette = "Set1")


p2 <- ggplot(data, aes(x=Fare, fill = Sex)) +
            geom_density(alpha = 1.) +
            theme_bw() +
            labs(title = "Fare by sex" ,
                    x = "Passenger Fare",
                     y = "Density", color = "") +
            theme(legend.position = c(0.7,0.8),
                    axis.title = element_text(size = 16.0),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) +
            scale_fill_brewer(palette = "Set1")

p3 <- ggplot(train_set, aes(x=Age, fill = factor(Survived))) +
            geom_density(alpha = 1.) +
            theme_bw() +
            labs(title = "Age by Survival " ,
                    x = "Passenger Age",
                     y = "Density", color = "") +
            theme(legend.position = c(0.7,0.8),
                    axis.title = element_text(size = 16.0),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) +
            scale_fill_brewer(palette = "Set1")

p4 <- ggplot(data, aes(x=Age, fill = Sex)) +
            geom_density(alpha = 1.) +
            theme_bw() +
            labs(title = "Age by sex" ,
                    x = "Passenger Age",
                    y = "Density", color = "") +
            theme(legend.position = c(0.7,0.8),
                    axis.title = element_text(size = 16.0),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) +
            scale_fill_brewer(palette = "Set1")


multiplot(p1,p2,p3,p4,cols=2)
```


1. Survived vs deceased comparison by passenger class show interesting findings. Obviously, among 1st class passengers more people
survived than deceased. Besides, if we compare number of survived over all classes first class seem to dominate. This suggests that
even at that time wealthy had advantage over the poor. 

2. By looking at peaks of distributions of ages histogramed by survival as a factor, we can conclude that average age of survived was
less than the average age of deceased.  Mean age Survived = `r mean(train_set[train_set$Survived==1,]$Age)`, mean age deceased = 
`r mean(train_set[train_set$Survived==0,]$Age)`.


3. On average female tickets `r mean(data[data$Sex=='female',]$Fare)` were almost twice more expansive than male tickets
`r mean(data[data$Sex=='male',]$Fare)`. This is a little unexpected. One rationalization could be that more females embarked on
1st class than other lower classes, which increased the mean. 

4. Females were younger than males on average. 

## Machine Learning

I used "caret" package with multicore processing to speed up 8-fold cross validation. 
It has a lot of ML methods, which can be accessed via following command "names(getModelInfo())".
Training accuracy reported is the accuracy over entire training set. Repeated cross validation
uses repeatedly re-sampled subsets to validated the model.   

```{r, message=FALSE, warning=FALSE}
## setting seed for random number generator
seed <- 2017
set.seed(seed)
n_cores <- 8
registerDoMC(cores = n_cores)
n_resamples <- n_cores
n_repeats <- 4

## creating a data partition
#train_indx <- createDataPartition(y=train_set$Survived, p = 0.70, list=FALSE)
#Training <- train_set[train_indx, ]
#CrossVal <- train_set[-train_indx, ]
Training <- train_set
```


## K-Nearest Neighbor 

Knn algorithm classifies data points based on K closest neighbors in distance, where
over represented class gets the vote. K value and distance metric are the only
parameters to tune for cross validation. It is slow with large and high dimensional data.
Best K value can be found via elbow method(k value is chosen based on location where cross validation
RMSE converge), or based on peak cross validation accuracy. 

```{r knn, eval=TRUE, message=FALSE, warning=FALSE, fig.width=6}

list_length <- n_resamples*n_repeats + 1
seeds <- vector(mode = "list", length = list_length)
tuneLength <- 20
for(i in 1:list_length) seeds[[i]] <- sample.int(1000, tuneLength)

knnCtrl <- trainControl(method = "repeatedcv",
                     number = n_resamples,
                     repeats = n_repeats,
                     seeds = seeds,
                     allowParallel = TRUE)

# apply knn to training set
knnFit <- train(factor(Survived) ~ . ,
                    data = Training,
                    method = "knn",
                    metric = "Accuracy",
                    trControl = knnCtrl,
                    preProcess = c("center","scale"),
                    tuneLength = tuneLength)

#plot cross validation curve
plot(knnFit)
knn_best_tune <- knnFit$bestTune
knn_cv_acc <- max(knnFit$results$Accuracy)
```

As depicted above, the best number for nearest neighbor is `r knn_best_tune` that gave highest
cross validation accuracy `r knn_cv_acc`. Algorithm automatically chooses best fit for further processing.      


## Decision Tree Classification with Post Prunnig 

Here I used decision tree classifier under "rpart" package. Information gain measure is used to
determine which feature is given a priority for splitting at each node. To prevent over fitting(PtreeFit)
I performed post pruning, which does in turn score better than original tree(treeFit) on testing set.

```{r tree, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, fig.width=6}
#building a model with trees
treeFit <- rpart(factor(Survived) ~.,
                     data = Training,
                     method = "class",
                     parms = list(split = "information"),
                     #prior = c(.55,.45)),
                     control = rpart.control(minsplit = 5, cp = 0)) 

#plot xerror vs complexity parameter for original tree
plotcp(treeFit)

#tree_cv_acc <- 1 - min(treeFit$cptable[,3])
```

How does post pruning work? Basically first we grow the tree and fit our training set. Then we start from the
bottom and check if removing a node(where split happens) will affect prediction accuracy. If elimination of a node
reduces the accuracy on validation set then we keep it, otherwise we remove it and proceed. These procedure is suppose 
to reduce tree size and help reduce over fitting. In order to do pruning program needs complexity parameter that corresponds 
to a minimum xeror(cross validation error).  

```{r tree_diagram, message=FALSE, warning=FALSE,fig.width=12,fig.height=8}
## plotting tree diagram
rpart.plot(treeFit,
            fallen.leaves = FALSE,
            shadow.col = "gray",
            sub = "Tree Diagram")

```

Let's prune it.

```{r Ptree, message=FALSE, warning=FALSE, fig.width=10, fig.height=6}
## apply prunning using optimum values for "cp" and "nsplit" 
cp <- treeFit$cptable[which.min(treeFit$cptable[,"xerror"]),"CP"]
PtreeFit <- prune(treeFit, cp = cp)

## plotting pruned tree diagram
rpart.plot(PtreeFit,
            fallen.leaves = FALSE,
            cex = 0.3,
        #    tweak = 2,
            shadow.col = "gray",
            sub = "Pruned Tree Diagram")
```

We can see that pruned tree is much smaller than the original in size, and later we will see that it performs better
on testing set than the original fit. 


## Gradient Boosting "gbm"

Boosting is an ensemble method that combine multiple weak learners to create strong learner [link to good tutorial](https://gormanalysis.com/gradient-boosting-explained/).  


```{r GradBoost, message=FALSE, warning=FALSE,fig.width=10,fig.height=8}
gbmGrid <-  expand.grid(shrinkage = c(0.1,0.2,0.3,0.4),
                        interaction.depth = c(3,4,5,6), 
                        n.trees = (2:25)*2,
                        n.minobsinnode = 10)

gbmCtrl <- trainControl(method = "repeatedcv",
                           number = n_resamples,
                           repeats = n_repeats,
                           seeds = seeds,
                           allowParallel = TRUE)

gbmFit <- train(factor(Survived) ~ .,
                        data = Training, 
                        method = "gbm", 
                        trControl = gbmCtrl, 
                        verbose = FALSE, 
                        tuneGrid = gbmGrid)

plot(gbmFit)
```


```{r extreme boosting, message=FALSE, warning=FALSE, eval=F, echo=F,fig.width=10,fig.height=5}
## xgBoost Trees
xgbctrl <- trainControl(method = "repeatedcv",
                        repeats = n_repeats,
                        number = n_resamples, 
                        #summaryFunction = twoClassSummary,
                        #classProbs = TRUE,
                        allowParallel=T)
                       
xgbgrid <- expand.grid(nrounds = c(100,200,400),#,1000),
                       eta = c(0.1,0.2,0.3,0.4),
                       max_depth = c(2,4), #,6,8,10),
                       gamma = 1,
                       colsample_bytree = 0.7,
                       min_child_weight = 1,
                       subsample = 0.8)

xgbFit <-train( factor(Survived) ~. , 
                     data = Training,
                     method = "xgbTree",
                     trControl = xgbctrl,
                     tuneGrid = xgbgrid)

plot(xgbFit)
xgb_best_tune <- xgbFit$bestTune
xgb_best_acc <- max(xgbFit$results$Accuracy)
```


##ML: Random Forest

Random Forest is basically a decision tree method with bagging(bootstrap aggregating). Bagging is a resampling technique, where
subset of data is drown from original data with replacement many many times. By training a model on each resampled equal size
subset we get a lot of distinct models that can be built into strong learner when combined. In a Random Forest algorithm, 
we grow a tree on each resampled N subsets of data and later aggregate finale fit of all those trees. Averaging multiple models
have similar bias as each of the models on its own, and reduced variance because of the average taken. 
Here I used custom made function for Random Forest classifier in order to have repeated cross validation via grid search under caret
package[https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r](link). 


```{r Random Forest, eval=TRUE, message=FALSE, warning=FALSE,fig.width=6}

#Here we create custom rf method repeatedcv method is not implemented with it 
#####################################################################################
customRF <- list(type = "Classification",
                 library = "randomForest",
                 loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
                                  class = rep("numeric", 2),
                                  label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
                randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
#######################################################################################


for(i in 1:list_length) seeds[[i]] <- sample.int(1000, 60) # 4*15=60
tunegrid <- expand.grid(.mtry = c(1:15),
                        .ntree=c(500, 1000, 1500, 2000))
rfCtrl <- trainControl(method = "repeatedcv",
                     number = n_resamples,
                     repeats = n_repeats,
                     seeds = seeds,
                     allowParallel = TRUE)

rfFit <- train(factor(Survived) ~., 
               data = Training,
               method = customRF,
               metric = "Accuracy",
               tuneGrid = tunegrid,
               trControl = rfCtrl)

plot(rfFit)
rf_cv_acc <-  max(rfFit$results$Accuracy)
rf_best_tune <- rfFit$bestTune
```




## Neural Network Model

```{r nnet, message=FALSE, warning=FALSE,fig.width=6}
decay = seq(from = 0.1, to = 0.9, by = 0.1)
hidden_net_size = seq(from = 4, to = 18, by = 2)
n_weight_decay <- length(decay)
n_hidden_units <- length(hidden_net_size)
for(i in 1:list_length) seeds[[i]] <- sample.int(1000, n_hidden_units*n_weight_decay)

nnetCtrl <- trainControl(method = "repeatedcv", 
                           number = n_resamples, 
                           repeats = n_repeats, 
                           classProbs = TRUE, 
                           summaryFunction = twoClassSummary,
                           seeds = seeds,
                           allowParallel = TRUE)

nnetGrid <- expand.grid(size = hidden_net_size,
                         decay = decay)

nnetFit <- train(make.names(Survived) ~ ., data = Training,
                 method = "nnet",
                 metric = "ROC",
                 trControl = nnetCtrl,
                 preProcess = c("center","scale"),
                 tuneGrid = nnetGrid,
                 verbose = FALSE,
                 trace = FALSE)

plot(nnetFit)
nnet_best_tune <- nnetFit$bestTune
nnet_cv_roc <- max(nnetFit$results$ROC)
```


## SVM Radial Kernel

```{r svm_rk, message=FALSE, warning=FALSE,fig.width=6}

## Fit Radial Kernel----------------------------------------------------------
SVMgridRad <- expand.grid(C = (1:30)*0.1 + 0.2,
                          sigma = c(0.01,0.02,0.03))

## Support Vector Machines with radial kernel
SVMctrl <- trainControl(method = "repeatedcv",
                        number = n_resamples,
                        repeats = n_repeats,
                        verbose = FALSE,
                        allowParallel = TRUE)

SVMFit_rk <- train(factor(Survived) ~ .,
                     data = Training, 
                     method = "svmRadial",
                     trControl = SVMctrl,
                     tuneGrid = SVMgridRad,
                     preProc = c("scale","center"),
                     verbose = FALSE)
plot(SVMFit_rk)
svmr_best_tune <- SVMFit_rk$bestTune
svmr_cv_acc <- SVMFit_rk$results$Accuracy
```


## SVM Linear Kernel

```{r svm_lk, message=FALSE, warning=FALSE}

## Fit SVM Linear Kernel
SVMgridLin <- expand.grid(C = (1:10)*0.2 + 0.5) 

SVMFit_lk <- train(factor(Survived) ~ .,
                      data = Training, 
                      method = 'svmLinear',
                      trControl = SVMctrl,
                      tuneGrid = SVMgridLin,
                      preProc = c("scale","center"),
                      verbose = FALSE)
#plot(SVMFit_lk)
svml_best_tune <- SVMFit_rk$bestTune
svml_cv_acc <- SVMFit_rk$results$Accuracy
```


```{r Predictions, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

#predicting cv set
train_pred_boost <- predict(gbmFit, Training, type = "raw")
train_pred_knn <- predict(knnFit, Training, type = "raw")
train_pred_tree <- predict(treeFit, Training, type = "class")
train_pred_Ptree <- predict(PtreeFit, Training, type = "class")
train_pred_rf <- predict(rfFit, Training, type = "raw")
train_pred_nnet <- predict(nnetFit, Training, type = "raw")
train_pred_svmr <- predict(SVMFit_rk, Training, type="raw")
train_pred_svml <- predict(SVMFit_lk, Training, type="raw")
#train_pred_xgb <- predict(xgbFit, Training, type="raw")


#combine all cv predictions into 1 data frame
TrainPredDF <- data.frame("knn" = as.numeric(train_pred_knn) -1,
                       "tree" = as.numeric(train_pred_tree) -1,
                       "Ptree" = as.numeric(train_pred_Ptree) -1,
                       "boost" = as.numeric(train_pred_boost) -1,
                       "rf" = as.numeric(train_pred_rf) -1,
                       "nnet" = as.numeric(train_pred_nnet) -1,
                       "svmR" = as.numeric(train_pred_svmr) -1,
                       "svmL" = as.numeric(train_pred_svml) -1)
#                       "xboost" = as.numeric(train_pred_xgb) - 1)


#predicting test set
test_pred_boost <- predict(gbmFit, test_set, type = "raw")
test_pred_knn <- predict(knnFit, test_set, type = "raw")
test_pred_tree <- predict(treeFit, test_set, type = "class")
test_pred_Ptree <- predict(PtreeFit, test_set, type = "class")
test_pred_rf <- predict(rfFit, test_set, type = "raw")
test_pred_nnet <- predict(nnetFit, test_set, type = "raw")
test_pred_svmr <- predict(SVMFit_rk, test_set, type="raw")
test_pred_svml <- predict(SVMFit_lk, test_set, type="raw")
#test_pred_xgb <- predict(xgbFit, test_set, type="raw")

TestPredDF <- data.frame("knn" = as.numeric(test_pred_knn) -1,
                         "tree" = as.numeric(test_pred_tree) -1,
                         "Ptree" = as.numeric(test_pred_Ptree) -1,
                         "boost" = as.numeric(test_pred_boost) -1,
                         "rf" = as.numeric(test_pred_rf) -1,
                         "nnet" = as.numeric(test_pred_nnet) -1,
                         "svmR" = as.numeric(test_pred_svmr) -1,
                         "svmL" = as.numeric(test_pred_svml) -1)
#                         "xboost" = as.numeric(test_pred_xgb) - 1)

```



```{r CorrPlot, message=FALSE, warning=FALSE, fig.width=12, fig.height=5}
#visualize cv and test predictions via correlation plots
par(mfrow = c(1,2), font = 2, font.main= 10, cex = 0.9)
corrplot.mixed(cor(TestPredDF))
title(main = "Test set", adj = 0.5)
corrplot.mixed(cor(TrainPredDF))
title(main = "Train set", adj = 0.5)
```


```{r combineViaGam, message=FALSE, warning=FALSE, eval=FALSE, echo=FALSE}
#ensemble rediction
TrainPredDF$Survived <- train_set$Survived
train_combFit <- train(factor(Survived) ~. , method = "gam", data = TrainPredDF)
train_pred_comb <- predict(train_combFit, TrainPredDF, type = "raw")
test_pred_comb <- predict(train_combFit, TestPredDF, type = "raw")
```


```{r, message=FALSE, warning=FALSE,fig.height=5,fig.width=10}
#combine all train and test scores into 1 df
results <- data.frame(knn = c(confusionMatrix(train_pred_knn, Training$Survived)$overall[1],0.78947),
            tree = c(confusionMatrix(train_pred_tree, Training$Survived)$overall[1],0.73205),
            Ptree = c(confusionMatrix(train_pred_Ptree, Training$Survived)$overall[1],0.79425),
            boost = c(confusionMatrix(train_pred_boost, Training$Survived)$overall[1],0.77033),
            rf = c(confusionMatrix(train_pred_rf, Training$Survived)$overall[1],0.78947),
            nnet = c(confusionMatrix(factor(as.numeric(train_pred_nnet)-1), Training$Survived)$overall[1],0.77990),
            svmr = c(confusionMatrix(train_pred_svmr, Training$Survived)$overall[1],0.79904),
            svml = c(confusionMatrix(train_pred_svml, Training$Survived)$overall[1],0.78468),
#            xgb = c(confusionMatrix(train_pred_xgb, Training$Survived)$overall[1],0.74162),
            row.names = c("train","test"))%>%t()%>%data.frame()
results$method <- rownames(results)

#barplot all scores
gather(results,train, test, -method, key = "type", value = "scores")%>%
        ggplot(aes(factor(method), scores, fill = type)) + 
            geom_bar(stat="identity", position = "dodge",width = 0.5) + 
            theme_bw() +
            labs(title = "", x = "Method", y = "Scores", color="") +
            scale_fill_brewer(palette = "Set1")

```        


```{r majority vote, message=FALSE, warning=FALSE}
tmp <- rowSums(TestPredDF) / ncol(TestPredDF)
thresh_val <- 0.6
tmp[tmp >= thresh_val] <- 1
tmp[tmp < thresh_val] <- 0
test_pred_majority <- tmp
```


```{r SubmissionPrep, message=FALSE, warning=FALSE,eval=FALSE,echo=FALSE}
## storing and outputing the predicting into csv file
test_set$PassengerId <- row.names(test_set)
prediction <- test_pred_majority
output <- data.frame(PassengerId = test_set$PassengerId, Survived = prediction)
write.csv(output, file = 'maj_prediction.csv', row.names = F)
```

## Summary




