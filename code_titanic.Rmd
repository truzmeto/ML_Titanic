---
title: "Titanic Project"
author: "tar159"
date: "December 15, 2017"
output:
    html_document:
        code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
options(warn = -1)
```

## Introduction


```{r, echo=TRUE, message=FALSE, warning=FALSE}
## importing libraries
library("knitr")
library("ggplot2")
library("lattice") 
library("caret")
library("rpart")
library("rpart.plot")
library("RColorBrewer")
library("rattle")
library("randomForest")
library("lattice")
library("plyr")
library("Rmisc")
library("party")
library("doMC")
library("corrplot")

## loading locally stored data
test_set <- read.csv("../data/test.csv", na.strings = c("NA",""))
train_set <- read.csv("../data/train.csv", na.strings = c("NA",""))

test_set$Survived <- 2
## combining train and test sets
data <- rbind(train_set, test_set)
```


## Data Cleaning & Feature Extraction

```{r}
#counting percentage of NA's for each feature
round(colSums(is.na(data))/nrow(data) *100,2)
```

Cabin feature have 77.1% missing values, so let's ignore it.

```{r}
## removing Cabin features from both train and test sets
data$Cabin <- NULL
```


```{r}
## creating a new feature by extracting title from names, where we replace
## everything before ', ' and after '.' with '' (empty char.)
data$Titles <- gsub('(.*[,] )|([.].*)', '', data$Name)
```


```{r}
## tabulating titles grouped by sex
table(data$Sex, data$Titles)

## taking care of rare female titles
data$Titles[data$Titles == 'Ms'] <- 'Miss' 
data$Titles[data$Titles == 'Mme'] <- 'Mrs' 
data$Titles[data$Titles == 'Mlle'] <- 'Miss' 
data$Titles[data$Titles == 'Dona'] <- 'Miss' 
data[data$Titles == 'Lady',]$Titles <- 'Mrs'
data[data$Titles == 'the Countess',]$Titles <- 'Mrs'
data[data$Sex == 'female' & data$Titles == 'Dr',]$Titles <- 'Miss'

## renaming all rare male titles with most common male title
minor_title_male <- c('Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
data$Titles[data$Titles %in% minor_title_male]  <- 'Mr'
```

This new feature is gonna be very handy to estimate missing ages for passengers!

```{r, message=FALSE, warning=FALSE}
## here we construct values for missing ages by taking
## a mean for each name title. May not be the best way!
mean_age_by_title <- aggregate(Age ~ Titles, data = data, mean)
data[is.na(data$Age) & data$Titles == 'Master' ,]$Age <- mean_age_by_title$Age[1]
data[is.na(data$Age) & data$Titles == 'Miss' ,]$Age <- mean_age_by_title$Age[2]
data[is.na(data$Age) & data$Titles == 'Mr' ,]$Age <- mean_age_by_title$Age[3]
data[is.na(data$Age) & data$Titles == 'Mrs' ,]$Age <- mean_age_by_title$Age[4]

## filling missing fare by mean of passenger class
mean_fare_by_pclass <- aggregate(Fare ~ Pclass, data = data, mean)
data[is.na(data$Fare),]$Fare <- mean_fare_by_pclass$Fare[3]

## imputing missing embarkation type with highest embarkation class
data[is.na(data$Embarked),]$Embarked <- "S"

```

```{r, message=FALSE, warning=FALSE}
kable(table(data$Sex, data$Titles), align = "l")
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
data$AdultTeenChi[data$Age <= 12.0] <- "Child"
data$AdultTeenChi[data$Age > 12.0 & data$Age <= 20.0] <- "Teen"
data$AdultTeenChi[data$Age > 20.0 & data$Age <= 35.0] <- "YoungAdult"
data$AdultTeenChi[data$Age > 35.0 & data$Age <= 50.0] <- "MiddleAged"
data$AdultTeenChi[data$Age > 50.0] <- "Siniors"
data$AdultTeenChi <- as.factor(data$AdultTeenChi)
kable(table(data$AdultTeenChi, data$Titles), align = "l")
```


```{r, message=FALSE, warning=FALSE}
## recollecting the data back to train and test sets after
## cleaning and imputations are done
data$Titles <- as.factor(data$Titles)
data$Name <- NULL
data$Ticket <- NULL
test_set <- data[data$PassengerId >= 892,]
train_set <- data[data$PassengerId < 892,]
test_set$Survived <- NULL
```

## Exploratory Analysis
```{r, eval=FALSE, echo=FALSE, fig.height=4, fig.width=10, message=FALSE, warning=FALSE}

p1 <- ggplot(train_set, aes(x= Pclass, fill= factor(Survived))) +
            geom_histogram(binwidth=0.4, position="dodge")+
            theme_bw() +
            labs(title = "Survival by passenger class" ,
                    x="Passenger class", color="") +
            theme(axis.title = element_text(size = 16.0),
                    legend.position = c(0.3,0.8),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) 


p2 <- ggplot(data, aes(x=Fare, fill = Sex)) +
            geom_density(alpha = 1.) +
            theme_bw() +
            labs(title = "Survival by sex" ,
                    x = "Passenger sex",
                     y = "Density", color = "") +
            theme(legend.position = c(0.7,0.8),
                    axis.title = element_text(size = 16.0),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) 

multiplot(p1,p2,cols=2)
```

## Machine Learning

```{r, message=FALSE, warning=FALSE}
## setting seed for random number generator
seed <- 2017
set.seed(seed)

## creating a data partition
train_indx <- createDataPartition(y=train_set$Survived, p = 0.70, list=FALSE)
Training <- train_set[train_indx, ]
CrossVal <- train_set[-train_indx, ]

```



```{r knn, eval=TRUE, message=FALSE, warning=FALSE}
n_cores <- 8
registerDoMC(cores = n_cores)
set.seed(seed)
n_resamples <- n_cores
n_repeats <- 4
list_length <- n_resamples*n_repeats + 1
seeds <- vector(mode = "list", length = list_length)
tuneLength <- 20
for(i in 1:list_length) seeds[[i]] <- sample.int(1000, tuneLength)

knnCtrl <- trainControl(method = "repeatedcv",
                     number = n_resamples,
                     repeats = n_repeats,
                     seeds = seeds,
                     allowParallel = TRUE)

# apply knn to training set
knnFit <- train(factor(Survived) ~ . ,
                    data = Training,
                    method = "knn",
                    metric = "Accuracy",
                    trControl = knnCtrl,
                    preProcess = c("center","scale"),
                    tuneLength = tuneLength)

#plot cross validation curve
plot(knnFit)

# predict with knn
#cv_pred_knn <- predict(knnFit, newdata = CrossVal)
#con_mat <- confusionMatrix(cv_pred_knn, CrossVal$Survived)
#knn_acc <- con_mat$overall[1]

```



```{r tree, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
## building a model with trees
treeFit <- rpart(factor(Survived) ~.,
                     data = Training,
                     method = "class",
                     parms = list(split = "information"),
                     #prior = c(.55,.45)),
                     control = rpart.control(minsplit = 5, cp = 0)) 

## plot xerror vs complexity parameter for original tree
plotcp(treeFit)

## apply prunning using optimum values for "cp" and "nsplit" 
cp <- treeFit$cptable[which.min(treeFit$cptable[,"xerror"]),"CP"]
pruned_treeFit <- prune(treeFit, cp = cp)

## plotting pruned tree diagram
library(rpart.plot)
rpart.plot(pruned_treeFit,
            fallen.leaves = FALSE,
            cex = 0.2,
            tweak = 2,
            shadow.col = "gray",
            sub = "Pruned Tree Diagram")

## predicting with pruned tree on validation set
#cv_pred_tree <- predict(pruned_treeFit, CrossVal, type = "class")
#con_mat_pruned_tree <- confusionMatrix(cv_pred_tree, CrossVal$Survived)
#tree_acc <- con_mat_pruned_tree$overall[1]
treeFit <- pruned_treeFit
```


## Stochastic grad boosting

```{r GradBoost, message=FALSE, warning=FALSE}
gbmGrid <-  expand.grid(shrinkage = c(0.1,0.2,0.3,0.4),
                        interaction.depth = c(3,4,5,6), 
                        n.trees = (2:50)*2,
                        n.minobsinnode = 10)

gbmCtrl <- trainControl(method = "repeatedcv",
                           number = n_resamples,
                           repeats = n_repeats,
                           seeds = seeds,
                           allowParallel = TRUE)

gbmFit <- train(factor(Survived) ~ .,
                        data = Training, 
                        method = "gbm", 
                        trControl = gbmCtrl, 
                        verbose = FALSE, 
                        tuneGrid = gbmGrid)

#best_n_trees <- gbmFit$bestTune$n.trees
#best_int_depth <- gbmFit$bestTune$interaction.depth
#best_shrink <- gbmFit$bestTune$shrinkage
plot(gbmFit)

## making a prediction
#cv_pred_boost <- predict(gbmFit, CrossVal, type = "raw")
#con_mat <- confusionMatrix(cv_pred_boost, CrossVal$Survived)
#boost_acc <- con_mat$overall[1]
```


##ML: Random Forest

```{r Random Forest, eval=TRUE, message=FALSE, warning=FALSE}

#####################################################################################
customRF <- list(type = "Classification",
                 library = "randomForest",
                 loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
                                  class = rep("numeric", 2),
                                  label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
                randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
#######################################################################################


for(i in 1:list_length) seeds[[i]] <- sample.int(1000, 60) # 4*15=60
tunegrid <- expand.grid(.mtry = c(1:15),
                        .ntree=c(500, 1000, 1500, 2000))
rfCtrl <- trainControl(method = "repeatedcv",
                     number = n_resamples,
                     repeats = n_repeats,
                     seeds = seeds,
                     allowParallel = TRUE)

rfFit <- train(factor(Survived) ~., 
               data = Training,
               method = customRF,
               metric = "Accuracy",
               tuneGrid = tunegrid,
               trControl = rfCtrl)

plot(rfFit)
## making a prediction
#cv_pred_rf <- predict(rfFit, CrossVal, type = "raw")
## checking performance on CV set
#con_mat <- confusionMatrix(cv_pred_rf, CrossVal$Survived)
#rf_acc <- con_mat$overall[1]
```


## Neural Network Model

```{r nnet}
for(i in 1:list_length) seeds[[i]] <- sample.int(1000, 64) #n_units*n_decay=8*8=64
nnetCtrl <- trainControl(method = "repeatedcv", 
                           number = n_resamples, 
                           repeats = n_repeats, 
                           classProbs = TRUE, 
                           summaryFunction = twoClassSummary,
                           seeds = seeds,
                           allowParallel = TRUE)

nnetGrid <- expand.grid(size = seq(from = 4, to = 18, by = 2),
                         decay = seq(from = 0.1, to = 0.8, by = 0.1))

nnetFit <- train(make.names(Survived) ~ ., data = Training,
                 method = "nnet",
                 metric = "ROC",
                 trControl = nnetCtrl,
                 preProcess = c("center","scale"),
                 tuneGrid = nnetGrid,
                 verbose = FALSE)

#cv_pred_nnet <- predict(nnetFit, newdata = CrossVal, type = "raw")
#con_mat_nnet <- confusionMatrix(cv_pred_nnet, make.names(CrossVal$Survived))

#plot and save
plot(nnetFit)
#print(nnetFit)
#best_size <- as.numeric(nnetFit$bestTune[1])
#best_decay <- as.numeric(nnetFit$bestTune[2])

```



```{r Predictions, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

#predicting cv set
cv_pred_boost <- predict(gbmFit, CrossVal, type = "raw")
cv_pred_knn <- predict(knnFit, CrossVal, type = "raw")
cv_pred_trees <- predict(treeFit, CrossVal, type = "class")
cv_pred_rf <- predict(rfFit, CrossVal, type = "raw")
cv_pred_nnet <- predict(nnetFit, CrossVal, type = "raw")


#combine all cv predictions into 1 data frame
CVPredDF <- data.frame("knn" = as.numeric(cv_pred_knn) -1,
                       "trees" = as.numeric(cv_pred_trees) -1,
                       "boost" = as.numeric(cv_pred_boost) -1,
                       "rf" = as.numeric(cv_pred_rf) -1,
                       "nnet" = as.numeric(cv_pred_nnet) -1)


#predicting test set
test_pred_boost <- predict(gbmFit, test_set, type = "raw")
test_pred_knn <- predict(knnFit, test_set, type = "raw")
test_pred_trees <- predict(treeFit, test_set, type = "class")
test_pred_rf <- predict(rfFit, test_set, type = "raw")
test_pred_nnet <- predict(nnetFit, test_set, type = "raw")


TestPredDF <- data.frame("knn" = as.numeric(test_pred_knn) -1,
                         "trees" = as.numeric(test_pred_trees) -1,
                         "boost" = as.numeric(test_pred_boost) -1,
                         "rf" = as.numeric(test_pred_rf) -1,
                         "nnet" = as.numeric(test_pred_nnet) -1)


```


```{r CorrPlot, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
#CVPredDF$comb <- as.numeric(cv_pred_comb)
#visualize cv and test predictions via correlation plots
par(mfrow = c(1,2), font = 2, font.main= 10, cex = 0.8)
corrplot.mixed(cor(TestPredDF))
title(main = "Test set", adj = 0.3)
corrplot.mixed(cor(CVPredDF))
title(main = "Cross validation set", adj = 0.3)
```


```{r combineViaGam, message=FALSE, warning=FALSE}
CVPredDF$Survived <- CrossVal$Survived
cv_combFit <- train(factor(Survived) ~. , method = "gam", data = CVPredDF)
cv_pred_comb <- predict(cv_combFit, CVPredDF, type = "raw")
test_pred_comb <- predict(cv_combFit, TestPredDF, type = "raw")
```

```{r majority vote, message=FALSE, warning=FALSE}
tmp <- rowSums(TestPredDF) / ncol(TestPredDF)
tmp[tmp >= 0.5] <- 1
tmp[tmp < 0.5] <- 0
test_pred_majority <- tmp
```


```{r, message=FALSE, warning=FALSE}
## storing and outputing the predicting into csv file
prediction <- test_pred_majority
output <- data.frame(PassengerID = test_set$PassengerId, Survived = factor(a))
write.csv(output, file = 'ensemble_prediction.csv', row.names = F)
```






