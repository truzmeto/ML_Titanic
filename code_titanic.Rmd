---
title: "Machine Learning with Titanic Survival Data"
author: "tar159"
date: "December 15, 2017"
output:
  pdf_document: default
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
options(warn = -1)
```

## Introduction

I've decided to do some deeper machine learning after taking some advanced ML courses. Looks
like titanic data set is the best and most popular data to start with. Here, I will exploit some machine
learning methods such as "knn", "Dtree", "prunnedTree", "gboosting", "nnet", "SVM" and "rf" with
repeated cross validation that helps to choose best hyper parameter for an optimum prediction. Then, I combine
all predictions of test set to do majority vote. My finale prediction put me at top 8%, which is not bad for a
starter, but it is possible to push it even more up by combining more models. Most of the kernel I've come across
mainly elaborate on EDA part rather than ML. Thus, I decided to concentrate on ML more than other aspects.   


```{r lib&data loading, echo=TRUE, message=FALSE, warning=FALSE}
## importing libraries
library("knitr")
library("ggplot2")
library("lattice") 
library("caret")
library("rpart")
library("rpart.plot")
library("RColorBrewer")
library("rattle")
library("randomForest")
library("lattice")
library("plyr")
library("Rmisc")
library("party")
library("doMC")
library("corrplot")
library("dplyr")
library("tidyr")
library("rpart.plot")

## loading locally stored data
test_set <- read.csv("../data/test.csv", na.strings = c("NA",""))
train_set <- read.csv("../data/train.csv", na.strings = c("NA",""))

test_set$Survived <- 2
## combining train and test sets
data <- rbind(train_set, test_set)
```


## Preprocessing, Data Cleaning & Feature Extraction

My preprocessing of unclean data is very similar to what most people have done. I need to give a credit
to [Megan Risdal](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) since I learned some tricks from her kernel. 

```{r preprocessing, message=FALSE, warning=FALSE}
#counting percentage of NA's for each feature
#round(colSums(is.na(data))/nrow(data) *100,2)

#Cabin feature have 77.1% missing values, so let's ignore it.
# removing Cabin features from both train and test sets
data$Cabin <- NULL

## creating a new feature by extracting title from names, where we replace
## everything before ', ' and after '.' with '' (empty char.)
data$Titles <- gsub('(.*[,] )|([.].*)', '', data$Name)

## tabulating titles grouped by sex
#table(data$Sex, data$Titles)

## taking care of rare female titles
data$Titles[data$Titles == 'Ms'] <- 'Miss' 
data$Titles[data$Titles == 'Mme'] <- 'Mrs' 
data$Titles[data$Titles == 'Mlle'] <- 'Miss' 
data$Titles[data$Titles == 'Dona'] <- 'Miss' 
data[data$Titles == 'Lady',]$Titles <- 'Mrs'
data[data$Titles == 'the Countess',]$Titles <- 'Mrs'
data[data$Sex == 'female' & data$Titles == 'Dr',]$Titles <- 'Miss'

## renaming all rare male titles with most common male title
minor_title_male <- c('Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
data$Titles[data$Titles %in% minor_title_male]  <- 'Mr'

## here we construct values for missing ages by taking
## a mean for each name title. May not be the best way!
mean_age_by_title <- aggregate(Age ~ Titles, data = data, mean)
data[is.na(data$Age) & data$Titles == 'Master' ,]$Age <- mean_age_by_title$Age[1]
data[is.na(data$Age) & data$Titles == 'Miss' ,]$Age <- mean_age_by_title$Age[2]
data[is.na(data$Age) & data$Titles == 'Mr' ,]$Age <- mean_age_by_title$Age[3]
data[is.na(data$Age) & data$Titles == 'Mrs' ,]$Age <- mean_age_by_title$Age[4]

## filling missing fare by mean of passenger class
mean_fare_by_pclass <- aggregate(Fare ~ Pclass, data = data, mean)
data[is.na(data$Fare),]$Fare <- mean_fare_by_pclass$Fare[3]

## imputing missing embarkation type with highest embarkation class
data[is.na(data$Embarked),]$Embarked <- "S"

#kable(table(data$Sex, data$Titles), align = "l")

# age based feature ectraction
data$AdultTeenChi[data$Age <= 12.0] <- "Child"
data$AdultTeenChi[data$Age > 12.0 & data$Age <= 20.0] <- "Teen"
data$AdultTeenChi[data$Age > 20.0 & data$Age <= 35.0] <- "YoungAdult"
data$AdultTeenChi[data$Age > 35.0 & data$Age <= 50.0] <- "MiddleAged"
data$AdultTeenChi[data$Age > 50.0] <- "Siniors"
data$AdultTeenChi <- as.factor(data$AdultTeenChi)
#kable(table(data$AdultTeenChi, data$Titles), align = "l")

## recollecting the data back to train and test sets after
## cleaning and imputations are done
data$Titles <- as.factor(data$Titles)
data$Name <- NULL
data$Ticket <- NULL
test_set <- data[data$PassengerId >= 892,]
train_set <- data[data$PassengerId < 892,]
test_set$PassengerId <- NULL
train_set$PassengerId <- NULL
```


## Some statistics

```{r, eval=T, echo=T, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}

p1 <- ggplot(train_set, aes(x= Pclass, fill= factor(Survived))) +
            geom_histogram(binwidth=0.4, position="dodge")+
            theme_bw() +
            labs(title = "Survival by passenger class" ,
                    x="Passenger class", color="") +
            theme(axis.title = element_text(size = 16.0),
                    legend.position = c(0.3,0.8),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) +
            scale_fill_brewer(palette = "Set1")


p2 <- ggplot(data, aes(x=Fare, fill = Sex)) +
            geom_density(alpha = 1.) +
            theme_bw() +
            labs(title = "Fare by sex" ,
                    x = "Passenger Fare",
                     y = "Density", color = "") +
            theme(legend.position = c(0.7,0.8),
                    axis.title = element_text(size = 16.0),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) +
            scale_fill_brewer(palette = "Set1")

p3 <- ggplot(train_set, aes(x=Age, fill = factor(Survived))) +
            geom_density(alpha = 1.) +
            theme_bw() +
            labs(title = "Age by Survival " ,
                    x = "Passenger Age",
                     y = "Density", color = "") +
            theme(legend.position = c(0.7,0.8),
                    axis.title = element_text(size = 16.0),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) +
            scale_fill_brewer(palette = "Set1")

p4 <- ggplot(data, aes(x=Age, fill = Sex)) +
            geom_density(alpha = 1.) +
            theme_bw() +
            labs(title = "Age by sex" ,
                    x = "Passenger Age",
                    y = "Density", color = "") +
            theme(legend.position = c(0.7,0.8),
                    axis.title = element_text(size = 16.0),
                    axis.text = element_text(size=12),
                    plot.title = element_text(size = 18, hjust = 0.5),
                    text=element_text(family="Times New Roman")) +
            scale_fill_brewer(palette = "Set1")


multiplot(p1,p2,p3,p4,cols=2)
```


1. Survived vs deceased comparison by passenger class show interesting findings. Obviously, among 1st class passengers more people
survived than deceased. Besides, if we compare number of survived over all classes first class seem to dominate. This suggests that
even at that time wealthy had advantage over the poor. 

2. By looking at peaks of distributions of ages histogramed by survival as a factor, we can conclude that average age of survived was
less than the average age of deceased.  Mean age Survived = `r mean(train_set[train_set$Survived==1,]$Age)`, mean age deceased = 
`r mean(train_set[train_set$Survived==0,]$Age)`.

3. On average female tickets `r mean(data[data$Sex=='female',]$Fare)` were almost twice more expansive than male tickets
`r mean(data[data$Sex=='male',]$Fare)`. This is a little unexpected. One rationalization could be that more females embarked on
1st class than other lower classes, which increased the mean. 

4. Females were younger than males on average. 

## Machine Learning

I used "caret" package with multicore processing to speed up 8-fold cross validation. 
It has a lot of ML methods, which can be accessed via following command "names(getModelInfo())".
Training accuracy reported is the accuracy over entire training set. Repeated cross validation
uses repeatedly re-sampled subsets to validated the model.   

```{r, message=FALSE, warning=FALSE}
## setting seed for random number generator
seed <- 2017
set.seed(seed)
n_cores <- 8
registerDoMC(cores = n_cores)
n_resamples <- n_cores
n_repeats <- 4

## creating a data partition
#train_indx <- createDataPartition(y=train_set$Survived, p = 0.70, list=FALSE)
#Training <- train_set[train_indx, ]
#CrossVal <- train_set[-train_indx, ]
Training <- train_set
```


## K-Nearest Neighbor 

Knn algorithm classifies data points based on K closest neighbors in distance, where
over represented class gets the vote. K value and distance metric are the only
parameters to tune for cross validation. It is slow with large and high dimensional data.
Best K value can be found via elbow method(k value is chosen based on location where cross validation
RMSE converge), or based on peak cross validation accuracy. 

```{r knn, eval=TRUE, message=FALSE, warning=FALSE, fig.width=6}

list_length <- n_resamples*n_repeats + 1
seeds <- vector(mode = "list", length = list_length)
tuneLength <- 20
for(i in 1:list_length) seeds[[i]] <- sample.int(1000, tuneLength)

knnCtrl <- trainControl(method = "repeatedcv",
                     number = n_resamples,
                     repeats = n_repeats,
                     seeds = seeds,
                     allowParallel = TRUE)

# apply knn to training set
knnFit <- train(factor(Survived) ~ . ,
                    data = Training,
                    method = "knn",
                    metric = "Accuracy",
                    trControl = knnCtrl,
                    preProcess = c("center","scale"),
                    tuneLength = tuneLength)

#plot cross validation curve
plot(knnFit)
knn_best_tune <- knnFit$bestTune
knn_cv_acc <- max(knnFit$results$Accuracy)
```

As depicted above, the best number for nearest neighbor is `r knn_best_tune` that gave highest
cross validation accuracy `r knn_cv_acc`. Algorithm automatically chooses best fit for further processing.      


## Decision Tree Classification with Post Prunnig 

Here I used decision tree classifier under "rpart" package. Information gain measure is used to
determine which feature is given a priority for splitting at each node. To prevent over fitting(PtreeFit)
I performed post pruning, which does in turn score better than original tree(treeFit) on testing set.

```{r tree, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, fig.width=6}
#building a model with trees
treeFit <- rpart(factor(Survived) ~.,
                     data = Training,
                     method = "class",
                     parms = list(split = "information"),
                     #prior = c(.55,.45)),
                     control = rpart.control(minsplit = 5, cp = 0)) 

#plot xerror vs complexity parameter for original tree
plotcp(treeFit)

#tree_cv_acc <- 1 - min(treeFit$cptable[,3])
```

How does post pruning work? Basically first we grow the tree and fit our training set. Then we start from the
bottom and check if removing a node(where split happens) will affect prediction accuracy. If elimination of a node
reduces the accuracy on validation set then we keep it, otherwise we remove it and proceed. These procedure is suppose 
to reduce tree size and help reduce over fitting. In order to do pruning program needs complexity parameter that corresponds to a minimum xeror(cross validation error).  

```{r tree_diagram, message=FALSE, warning=FALSE,fig.width=12,fig.height=8}
## plotting tree diagram
rpart.plot(treeFit,
            fallen.leaves = FALSE,
            shadow.col = "gray",
            sub = "Tree Diagram")

```

Let's prune it.

```{r Ptree, message=FALSE, warning=FALSE, fig.width=6}
## apply prunning using optimum values for "cp" and "nsplit" 
cp <- treeFit$cptable[which.min(treeFit$cptable[,"xerror"]),"CP"]
PtreeFit <- prune(treeFit, cp = cp)

## plotting pruned tree diagram
rpart.plot(PtreeFit,
            fallen.leaves = FALSE,
            cex = 0.7,
            tweak = 1.5,
            shadow.col = "gray",
            sub = "Pruned Tree Diagram")
```

We can see that pruned tree is much smaller than the original in size, and later we will see that it performs better
on testing set than the original fit. 


## Stochastic Gradient Boosting "gbm"

Boosting is an ensemble method that uses linear weighted combination of multiple weak learners to create strong learner [link to good a good paper](http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/boosting-survey.pdf), [good video tutorial](https://www.youtube.com/watch?v=w75WyRjRpAg).
Algorithm iterativaly constructs set of weak classifiers such that every new hypothesis puts stronger emphasis on misclassified
samples and less emphasis on correctly classified samples. At the end we get strong learner. 
Here I used [Stochastic Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting), which is later modification to original boosting algorithm.
In this algorithm, at each iteration a weak learner is fit on a sub sample of the training set drawn randomly without
replacement.

Parameters: 

1. shrinkage - learning rate, how quickly the algorithm adapts

2. interaction.depth - complexity of the tree

3. n.trees - number of iterations(trees grown)

4. n.minobsinnode - the min number of training set samples in a node to commence splitting


```{r GradBoost, message=FALSE, warning=FALSE,fig.width=10,fig.height=8}
gbmGrid <-  expand.grid(shrinkage = c(0.1,0.2,0.3,0.4),
                        interaction.depth = c(3,4,5,6), 
                        n.trees = (2:25)*2,
                        n.minobsinnode = 50)

gbmCtrl <- trainControl(method = "repeatedcv",
                           number = n_resamples,
                           repeats = n_repeats,
                           seeds = seeds,
                           allowParallel = TRUE)

gbmFit <- train(factor(Survived) ~ .,
                        data = Training, 
                        method = "gbm", 
                        trControl = gbmCtrl, 
                        verbose = FALSE, 
                        tuneGrid = gbmGrid)

plot(gbmFit)
```



```{r extreme boosting, message=FALSE, warning=FALSE, eval=F, echo=F,fig.width=10,fig.height=5}
## xgBoost Trees
xgbctrl <- trainControl(method = "repeatedcv",
                        repeats = n_repeats,
                        number = n_resamples, 
                        #summaryFunction = twoClassSummary,
                        #classProbs = TRUE,
                        allowParallel=T)
                       
xgbgrid <- expand.grid(nrounds = c(100,200,400),#,1000),
                       eta = c(0.1,0.2,0.3,0.4),
                       max_depth = c(2,4), #,6,8,10),
                       gamma = 1,
                       colsample_bytree = 0.7,
                       min_child_weight = 1,
                       subsample = 0.8)

xgbFit <-train( factor(Survived) ~. , 
                     data = Training,
                     method = "xgbTree",
                     trControl = xgbctrl,
                     tuneGrid = xgbgrid)

plot(xgbFit)
xgb_best_tune <- xgbFit$bestTune
xgb_best_acc <- max(xgbFit$results$Accuracy)
```


##ML: Random Forest

Random Forest is basically a decision tree method with bagging(bootstrap aggregating). Bagging is a resampling technique, where
subset of data is drown from original data with replacement many many times. By training a model(tree) on each resampled equal size
subset we get a lot of distinct models that can be built into stronger learner when combined. In a Random Forest algorithm, 
we grow a tree on each resampled N subsets of data and later aggregate finale fit of all those trees. Averaging multiple models
have similar bias as each of the models on its own, and reduced variance because of the average taken. 
Here I used custom made function for Random Forest classifier in order to have repeated cross validation via grid search under caret
package [link](https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r). 

Parameters:

1. .mtry - number of variables randomly sampled at each split

2. n.tree - number of trees to grow


```{r Random Forest, eval=F,echo=F, message=FALSE, warning=FALSE,fig.width=6}

#Here we create custom rf method repeatedcv method is not implemented with it 
#####################################################################################
customRF <- list(type = "Classification",
                 library = "randomForest",
                 loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
                                  class = rep("numeric", 2),
                                  label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
                randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
#######################################################################################


for(i in 1:list_length) seeds[[i]] <- sample.int(1000, 60) # 4*15=60
tunegrid <- expand.grid(.mtry = c(1:15),
                        .ntree=c(100, 200, 500, 1000))
rfCtrl <- trainControl(method = "repeatedcv",
                     number = n_resamples,
                     repeats = n_repeats,
                     seeds = seeds,
                     allowParallel = TRUE)

rfFit <- train(factor(Survived) ~., 
               data = Training,
               method = customRF,
               metric = "Accuracy",
               tuneGrid = tunegrid,
               trControl = rfCtrl)

plot(rfFit)
rf_cv_acc <-  max(rfFit$results$Accuracy)
rf_best_tune <- rfFit$bestTune
```

__Unfortunately I had to disable RF model to keep run time under 3600 secs. If one needs to use it, it can easyly be enabled by setting in a code chuck option "eval=T". Adding it to rest of the models
will indeed improve prediction score a little bit__


## Neural Network Model

Neural Network uses backpropagation algorithm in order to get optimal values for all weights.
More hidden layers introduce more complexity into model but also resuts in more intensive computation.  

Hyper parameters:

1. number of hidden units

2. weight decay - is a regularization term that penalizes big weights and by doing so helps avoid over-fitting

```{r nnet, message=FALSE, warning=FALSE,fig.width=6}
decay = seq(from = 0.1, to = 0.9, by = 0.1)
hidden_net_size = seq(from = 4, to = 18, by = 2)
n_weight_decay <- length(decay)
n_hidden_units <- length(hidden_net_size)
for(i in 1:list_length) seeds[[i]] <- sample.int(1000, n_hidden_units*n_weight_decay)

nnetCtrl <- trainControl(method = "repeatedcv", 
                           number = n_resamples, 
                           repeats = n_repeats, 
                           classProbs = TRUE, 
                           summaryFunction = twoClassSummary,
                           seeds = seeds,
                           allowParallel = TRUE)

nnetGrid <- expand.grid(size = hidden_net_size,
                         decay = decay)

nnetFit <- train(make.names(Survived) ~ ., data = Training,
                 method = "nnet",
                 metric = "ROC",
                 trControl = nnetCtrl,
                 preProcess = c("center","scale"),
                 tuneGrid = nnetGrid,
                 verbose = FALSE,
                 trace = FALSE)

plot(nnetFit)
nnet_best_tune <- nnetFit$bestTune
nnet_cv_roc <- max(nnetFit$results$ROC)
```


## Support Vector Machine (SVM)

[Support Vector Machine](https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html)
is supervised learning algorithm used for classification problems.
It classifies instances by choosing an optimum decision boundary(can be both linear and non-linear)
with maximum margin, where length of the margin is half distance between two support vectors. 

Hyperparameters: 

1. C - is a regularization parameter that controls the trade off between the achieving a low training
   error and a low testing error that is the ability to generalize your classifier to unseen data

2. $\sigma$ - is related to standard deviation of Gaussian when we use radial basis function. 

## Radial Kernel

```{r svm_rk, message=FALSE, warning=FALSE,fig.width=6}

## Fit Radial Kernel----------------------------------------------------------
SVMgridRad <- expand.grid(C = (1:30)*0.1 + 0.2,
                          sigma = c(0.01,0.02,0.03))

## Support Vector Machines with radial kernel
SVMctrl <- trainControl(method = "repeatedcv",
                        number = n_resamples,
                        repeats = n_repeats,
                        verbose = FALSE,
                        allowParallel = TRUE)

SVMFit_rk <- train(factor(Survived) ~ .,
                     data = Training, 
                     method = "svmRadial",
                     trControl = SVMctrl,
                     tuneGrid = SVMgridRad,
                     preProc = c("scale","center"),
                     verbose = FALSE)
plot(SVMFit_rk)
svmr_best_tune <- SVMFit_rk$bestTune
svmr_cv_acc <- SVMFit_rk$results$Accuracy
```

## Linear Kernel

SVM with linear kernel uses linear decision boundary and good at classifying linearly separable
samples. 

```{r svm_lk, message=FALSE, warning=FALSE, echo=T,eval=T}

## Fit SVM Linear Kernel
SVMgridLin <- expand.grid(C = (1:10)*0.2 + 0.5) 

SVMFit_lk <- train(factor(Survived) ~ .,
                      data = Training, 
                      method = 'svmLinear',
                      trControl = SVMctrl,
                      tuneGrid = SVMgridLin,
                      preProc = c("scale","center"),
                      verbose = FALSE)
#plot(SVMFit_lk)
svml_best_tune <- SVMFit_rk$bestTune
svml_cv_acc <- SVMFit_rk$results$Accuracy
```


## Combine all models

Now all above models are cross validated by choosing the optimal hyperparameters. Next, we combine them
in order to generate majority vote. Note that, usually odd number of models should be combined to avoid
getting 50%-50% vote. Since, I have even number of models combined, I'll just put threshold at 0.6 .


```{r Predictions, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
#predicting cv set
train_pred_boost <- predict(gbmFit, Training, type = "raw")
train_pred_knn <- predict(knnFit, Training, type = "raw")
train_pred_tree <- predict(treeFit, Training, type = "class")
train_pred_Ptree <- predict(PtreeFit, Training, type = "class")
#train_pred_rf <- predict(rfFit, Training, type = "raw")
train_pred_nnet <- predict(nnetFit, Training, type = "raw")
train_pred_svmr <- predict(SVMFit_rk, Training, type="raw")
train_pred_svml <- predict(SVMFit_lk, Training, type="raw")
#train_pred_xgb <- predict(xgbFit, Training, type="raw")

#combine all cv predictions into 1 data frame
TrainPredDF <- data.frame("knn" = as.numeric(train_pred_knn) -1,
                       "tree" = as.numeric(train_pred_tree) -1,
                       "Ptree" = as.numeric(train_pred_Ptree) -1,
                       "boost" = as.numeric(train_pred_boost) -1,
#                       "rf" = as.numeric(train_pred_rf) -1,
                       "nnet" = as.numeric(train_pred_nnet) -1,
                       "svmR" = as.numeric(train_pred_svmr) -1,
                       "svmL" = as.numeric(train_pred_svml) -1)
#                       "xboost" = as.numeric(train_pred_xgb) - 1)


#predicting test set
test_pred_boost <- predict(gbmFit, test_set, type = "raw")
test_pred_knn <- predict(knnFit, test_set, type = "raw")
test_pred_tree <- predict(treeFit, test_set, type = "class")
test_pred_Ptree <- predict(PtreeFit, test_set, type = "class")
#test_pred_rf <- predict(rfFit, test_set, type = "raw")
test_pred_nnet <- predict(nnetFit, test_set, type = "raw")
test_pred_svmr <- predict(SVMFit_rk, test_set, type="raw")
test_pred_svml <- predict(SVMFit_lk, test_set, type="raw")
#test_pred_xgb <- predict(xgbFit, test_set, type="raw")

TestPredDF <- data.frame("knn" = as.numeric(test_pred_knn) -1,
                         "tree" = as.numeric(test_pred_tree) -1,
                         "Ptree" = as.numeric(test_pred_Ptree) -1,
                         "boost" = as.numeric(test_pred_boost) -1,
 #                        "rf" = as.numeric(test_pred_rf) -1,
                         "nnet" = as.numeric(test_pred_nnet) -1,
                         "svmR" = as.numeric(test_pred_svmr) -1,
                         "svmL" = as.numeric(test_pred_svml) -1)
#                         "xboost" = as.numeric(test_pred_xgb) - 1)

```

In order to see similarities of prediction on both test and train sets one can do correlation plot between models and plot them.


```{r CorrPlot, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
#visualize cv and test predictions via correlation plots
par(mfrow = c(1,2), font = 2, font.main= 10, cex = 0.9)
corrplot.mixed(cor(TestPredDF))
title(main = "Test set", adj = 0.5)
corrplot.mixed(cor(TrainPredDF))
title(main = "Train set", adj = 0.5)
```

Whole idea behind majority vote is, we expect our final model on average to do better than a chance. For instance, let's say I'm
combining 100 different model. Then in order to correctly predict some specific sample(row) in a test set we need at least 51 models
out of 100 to make a correct prediction.  

```{r combineViaGam, message=FALSE, warning=FALSE, eval=FALSE, echo=FALSE}
#ensemble rediction
TrainPredDF$Survived <- train_set$Survived
train_combFit <- train(factor(Survived) ~. , method = "gam", data = TrainPredDF)
train_pred_comb <- predict(train_combFit, TrainPredDF, type = "raw")
test_pred_comb <- predict(train_combFit, TestPredDF, type = "raw")
```

```{r majority vote, message=FALSE, warning=FALSE}
tmp <- rowSums(TestPredDF) / ncol(TestPredDF)
thresh_val <- 0.5
tmp[tmp >= thresh_val] <- 1
tmp[tmp < thresh_val] <- 0
test_pred_majority <- tmp
```



```{r, message=FALSE, warning=FALSE,fig.height=5,fig.width=10}
#combine all train and test scores into 1 df
results <- data.frame(knn = c(confusionMatrix(train_pred_knn, Training$Survived)$overall[1],0.78947),
            tree = c(confusionMatrix(train_pred_tree, Training$Survived)$overall[1],0.73205),
            Ptree = c(confusionMatrix(train_pred_Ptree, Training$Survived)$overall[1],0.79425),
            boost = c(confusionMatrix(train_pred_boost, Training$Survived)$overall[1],0.77033),
  #          rf = c(confusionMatrix(train_pred_rf, Training$Survived)$overall[1],0.78947),
            nnet = c(confusionMatrix(factor(as.numeric(train_pred_nnet)-1), Training$Survived)$overall[1],0.77990),
            svmr = c(confusionMatrix(train_pred_svmr, Training$Survived)$overall[1],0.79904),
            svml = c(confusionMatrix(train_pred_svml, Training$Survived)$overall[1],0.78468),
            row.names = c("train","test"))%>%t()%>%data.frame()
results$method <- rownames(results)

#barplot all scores
gather(results,train, test, -method, key = "type", value = "scores")%>%
        ggplot(aes(factor(method), scores, fill = type)) + 
            geom_bar(stat="identity", position = "dodge",width = 0.5) + 
            theme_bw() +
            labs(title = "", x = "Method", y = "Scores", color="") +
            scale_fill_brewer(palette = "Set1")

```        



```{r SubmissionPrep, message=FALSE, warning=FALSE,eval=TRUE,echo=T}
## storing and outputing the predicting into csv file
test_set$PassengerId <- row.names(test_set)
prediction <- test_pred_majority
output <- data.frame(PassengerId = test_set$PassengerId, Survived = prediction)
write.csv(output, file = 'maj_prediction.csv', row.names = F)
```


## What can be done next

1. Well, one can add more different type of models and increase test accuracy

2. Domain knowledge based feature engineering can help boost prediction[Oscar Takeshita](https://www.kaggle.com/pliptor/divide-and-conquer-0-82296) 


## Summary

1. Cross validation is a useful approach to reduce training error, but does not necessarily help avoid over fitting. 

2. Model specific cross validation via pruning the decision tree was able to balance bias and variance better than other hyper parameter
   tuning approaches. 

3. K-fold cross validation with repetitions can be extremely time consuming. One can accelerate the training by using very user friendly "doMC" library.  

3. Combining different models by majority vote is a recognized way of boosting testing score!


Feel free to leave a comment below and up vote if you liked it. I'll be more than happy to respond to any questions regarding the kernel. 




